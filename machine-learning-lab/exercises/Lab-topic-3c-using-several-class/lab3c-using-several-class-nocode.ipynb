{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Using several classifiers and tuning parameters - Parameters grid\n",
    "[From official `scikit-learn` documentation](http://scikit-learn.org/stable/auto_examples/model_selection/plot_grid_search_digits.html)\n",
    "\n",
    "Adapted by Claudio Sartori\n",
    "\n",
    "Example of usage of the ***model selection*** features of `scikit-learn` and comparison of several classification methods.\n",
    "1. import a sample dataset \n",
    "1. do the usual preliminary data explorations and separate the predicting attributes from the *target* `'Exited'`\n",
    "1. define the _models_ that will be tested and prepare the _hyperparameter ranges_ for the modules\n",
    "1. set the list of *score functions* to choose from\n",
    "1. split the dataset into two parts: train and test\n",
    "1. Loop on score functions and, for each score, loop on the model labels (see details below)\n",
    "    - optimize with GridSearchCV\n",
    "    - test\n",
    "    - store the results of best model\n",
    "1. for each scoring show the best models for each classifier, sorted by decreasing performance\n",
    "1. for each scoring show the confusion matrix of the prediction given by the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "@author: scikit-learn.org and Claudio Sartori\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "@author: scikit-learn.org and Claudio Sartori\n",
    "\"\"\"\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') # uncomment this line to suppress warnings\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import classification_report, ConfusionMatrixDisplay\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
    "\n",
    "print(__doc__) # print information included in the triple quotes at the beginning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Initial settings\n",
    "Set the random state and set the seed with `np.random.seed()`\n",
    "\n",
    "Set the test set size and the number of cross valitadion splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = 0.3 # test size\n",
    "random_state = 42\n",
    "np.random.seed(random_state) # this sets the random sequence. Setting only this the repeatability is guaranteed\n",
    "                             # only if we re-execute the entire notebook\n",
    "cv = 3   # number of cross-validation splits"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Tenure</th>\n",
       "      <th>Balance</th>\n",
       "      <th>NumOfProducts</th>\n",
       "      <th>HasCrCard</th>\n",
       "      <th>IsActiveMember</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "      <th>Exited</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>619</td>\n",
       "      <td>0</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101348.88</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>502</td>\n",
       "      <td>0</td>\n",
       "      <td>42</td>\n",
       "      <td>8</td>\n",
       "      <td>159660.80</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113931.57</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>699</td>\n",
       "      <td>0</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>93826.63</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>822</td>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>7</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10062.80</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>501</td>\n",
       "      <td>1</td>\n",
       "      <td>44</td>\n",
       "      <td>4</td>\n",
       "      <td>142051.07</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>74940.50</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   CreditScore  Gender  Age  Tenure    Balance  NumOfProducts  HasCrCard  \\\n",
       "0          619       0   42       2       0.00              1          1   \n",
       "1          502       0   42       8  159660.80              3          1   \n",
       "2          699       0   39       1       0.00              2          0   \n",
       "3          822       1   50       7       0.00              2          1   \n",
       "4          501       1   44       4  142051.07              2          0   \n",
       "\n",
       "   IsActiveMember  EstimatedSalary  Exited  \n",
       "0               1        101348.88    True  \n",
       "1               0        113931.57    True  \n",
       "2               0         93826.63   False  \n",
       "3               1         10062.80   False  \n",
       "4               1         74940.50   False  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url =  'churn-analysis.csv'\n",
    "#df = pd.read_csv(url)\n",
    "#print(df)\n",
    "df = pd.read_csv(url, sep = ',')\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Explore the data\n",
    "\n",
    "The output of exploration is not shown here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot some information about the dataset pairplot\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Exited\n",
       "False    4204\n",
       "True      810\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = 'Exited'\n",
    "#sns.pairplot(df, hue=target, height=2)\n",
    "X = df.drop(target, axis=1)\n",
    "y = df[target]\n",
    "y.value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Define the _models_ \n",
    "Prepare the _hyperparameter ranges_ for the modules\n",
    "\n",
    "Put everything in a dictionary, for ease of use\n",
    "\n",
    "Please read carefully these data structures and understand them\n",
    "\n",
    "It is also useful to read the API documentation of each classifier, in order to understand the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "model_lbls = ['dt' # decision tree\n",
    "             ,'nb' # gaussian naive bayes\n",
    "             ,'lp'   # linear perceptron\n",
    "               ,'svc'  # support vector # deactivate if running time becomes too long\n",
    "             ,'knn'  # k nearest neighbours\n",
    "             ,'adb'  # adaboost\n",
    "             ,'rf'   # random forest\n",
    "            ]\n",
    "\n",
    "models = {\n",
    "    'dt': {'name': 'Decision Tree       ',\n",
    "           'estimator': DecisionTreeClassifier(random_state=random_state), \n",
    "           'param': [{'max_depth': [*range(1,20)],'class_weight':[None,'balanced']}],\n",
    "          },\n",
    "    'nb': {'name': 'Gaussian Naive Bayes',\n",
    "           'estimator': GaussianNB(),\n",
    "           'param': [{'var_smoothing': [10**exp for exp in range(-3,-13,-1)]}]\n",
    "          },\n",
    "    'lp': {'name': 'Linear Perceptron   ',\n",
    "           'estimator': Perceptron(random_state=random_state),\n",
    "           'param': [{'early_stopping': [True,False],'class_weight':[None,'balanced']}],\n",
    "          },\n",
    "    'svc':{'name': 'Support Vector      ',\n",
    "           'estimator': SVC(random_state=random_state), \n",
    "           'param': [{'kernel': ['rbf'], \n",
    "                    'gamma': [1e-3, 1e-4],\n",
    "                    'C': [1, 10, 100],\n",
    "                    },\n",
    "                    {'kernel': ['linear'],\n",
    "                     'C': [1, 10, 100],                     \n",
    "             },\n",
    "                   ]\n",
    "          },\n",
    "    'knn':{'name': 'K Nearest Neighbor ',\n",
    "           'estimator': KNeighborsClassifier(),\n",
    "           'param': [{'n_neighbors': list(range(1,7))}]\n",
    "       },\n",
    "    'adb':{'name': 'AdaBoost           ',\n",
    "           'estimator': AdaBoostClassifier(random_state=random_state),\n",
    "           'param': [{'n_estimators':[10,20,30,40,50]\n",
    "                     ,'learning_rate':[0.2,0.5,0.75,1,1.25,1.5]}]\n",
    "          },\n",
    "    'rf': {'name': 'Random forest       ',\n",
    "           'estimator': RandomForestClassifier(random_state=random_state),\n",
    "           'param': [{'max_depth': [*range(4,10)]\n",
    "                     ,'n_estimators':[*range(10,60,10)]}]\n",
    "          }\n",
    "\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Set the list of *score functions* to choose from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "scorings = ['accuracy', 'precision_macro', 'recall_macro', 'f1_macro']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dt': 38, 'nb': 10, 'lp': 4, 'svc': 9, 'knn': 6, 'adb': 30, 'rf': 30}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import ParameterGrid\n",
    "trials = {lbl: len(list(ParameterGrid(models[lbl]['param']))) for lbl in models.keys()}\n",
    "trials"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Split the dataset into the train and test parts\n",
    "    - the *train* part will be used for training and cross-validation (i.e. for *development*)\n",
    "    - the *test* part will be used for test (i.e. for *evaluation*)\n",
    "    - the fraction of test data will be _ts_ (a value of your choice between 0.2 and 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 3509 examples.\n"
     ]
    }
   ],
   "source": [
    "# split the dataset in train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=ts)\n",
    "print(\"Training on {} examples.\".format(len(X_train)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Loop on scores and, for each score, loop on the model labels\n",
    "\n",
    "The function `GridSearchCV` iterates a cross validation experiment to train and test a model with different combinations of paramater values\n",
    "- for each parameter we have set before a list of values to test, `ParametersGrid` will be implicitly called to generate all the combinations\n",
    "- we choose a *score function* which will be used for the optimization\n",
    "    - e.g. `accuracy_score`, `precision_score`, `recall_score`, `f1_score`, see this [page](https://scikit-learn.org/stable/modules/model_evaluation.html) for reference\n",
    "- the output is a dataframe containing \n",
    "    - the set of parameters maximising the score \n",
    "    - the score used for optimisation and all the test scores\n",
    "\n",
    "### Steps\n",
    "\n",
    "- prepare an empty list `clfs` to store all the fitte models\n",
    "- prepare an empty DataFrame which will collect the results of the fittings with each combination of parameters\n",
    "    - dataframe columns are `'scoring','model','best_params','accuracy','precision_macro','recall_macro','f1_macro'`\n",
    "- loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfs = []\n",
    "results = pd.DataFrame(columns=['scoring','model','best_params'#,'fit+score_time'\n",
    "                                ,'accuracy','precision_macro','recall_macro','f1_macro'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters to collect\n",
    "\n",
    "`classification_report` produces a dictionary containing some classification performance measures, given the *ground truth* and the *predictions* (use the parameter `output_dict=True`)\n",
    "\n",
    "The measures are (among others):\n",
    "- `accuracy`\n",
    "- `macro avg` a dictionary containing:\n",
    "    - `precision`\n",
    "    - `recall`\n",
    "    - `f1-score`\n",
    "- ...\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop\n",
    "\n",
    "- repeat for all the chosen scorings\n",
    "    - repeat for all the chosen classification models\n",
    "        - store in `clf` the initialisation of `GridSearchCV` with the appropriate\n",
    "            - classification model\n",
    "            - parameters ranges\n",
    "            - scoring\n",
    "            - cross validation method `cv` (the same for all)\n",
    "        - fit `clf` with the *train* part of `X` and `y`\n",
    "        - store in `y_pred` the prediction for the *test* part of `X`\n",
    "        - append `clf` to clfs`\n",
    "        - append `y_pred` to `y_preds`\n",
    "        - store in variable `cr` the `classification_report` produced with the test part of `y` and `y_pred`\n",
    "        - store in the last row of `results` a list containing:\n",
    "            - the name of the model\n",
    "            - the `.best_params_` of `clf`\n",
    "            - a selection of the contents of cr\n",
    "                - 'accuracy', \n",
    "                - 'macro avg''precision'\n",
    "                - 'macro avg''recall'\n",
    "                - 'macro avg''f1-score'\n",
    "\n",
    "Hints: \n",
    "- cr is a multi-level dictionary, second level can be reached \n",
    "with <br> `cr['first level label']['second level label']`\n",
    "- to append a list as the last row of a dataframe you can use <br>\n",
    "`df.loc[len(df)]=[]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# loop over the score functions, then over the models + optimization with GridSearchCV + test + store results of the best model\n",
    "for score in scorings:\n",
    "    for m in model_lbls:\n",
    "        clf = GridSearchCV(models[m]['estimator'], models[m]['param'], cv=cv, scoring=score)\n",
    "        clf.fit(X_train, y_train)\n",
    "        clfs.append(clf)\n",
    "        y_true, y_pred = y_test, clf.predict(X_test)\n",
    "        cr = classification_report(y_true, y_pred, output_dict=True, zero_division=1)\n",
    "        results.loc[len(results)] = [score,models[m]['name'],clf.best_params_\n",
    "                                    # ,(clf.cv_results_['mean_fit_time'].sum()+clf.cv_results_['mean_score_time'].sum())*n_splits\n",
    "                                    ,cr['accuracy']\n",
    "                                    ,cr['macro avg']['precision']\n",
    "                                    ,cr['macro avg']['recall']\n",
    "                                    ,cr['macro avg']['f1-score']]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Display\n",
    "\n",
    "For each scoring show the ranking of the models, and the confusion matrix given by the best model\n",
    "\n",
    "For each scoring:\n",
    "- set a `scoring_filter`\n",
    "- filter the results of that scoring\n",
    "- display the filtered dataframe with the `display()` function (it allows several displays of dataframes )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>scoring</th>\n",
       "      <th>model</th>\n",
       "      <th>best_params</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision_macro</th>\n",
       "      <th>recall_macro</th>\n",
       "      <th>f1_macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [scoring, model, best_params, accuracy, precision_macro, recall_macro, f1_macro]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyError",
     "evalue": "'precision'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_16524\\559298714.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mscore\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mscore_list\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mscoring_filter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'scoring'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mdisplay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mscoring_filter\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33mf'\u001b[0m\u001b[1;33mBest model for \u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m is \u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mresults\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mscoring_filter\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mby\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mascending\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33mf'\u001b[0m\u001b[1;33mBest parameters for \u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m are \u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mresults\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mscoring_filter\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mby\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mascending\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33mf'\u001b[0m\u001b[1;33mBest \u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m for \u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m is \u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mresults\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mscoring_filter\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mby\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mascending\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Programs\\Python\\envs\\ML24\\Lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, by, axis, ascending, inplace, kind, na_position, ignore_index, key)\u001b[0m\n\u001b[0;32m   7185\u001b[0m             \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7186\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mby\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7187\u001b[0m             \u001b[1;31m# len(by) == 1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7188\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 7189\u001b[1;33m             \u001b[0mk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_label_or_level_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mby\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   7190\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7191\u001b[0m             \u001b[1;31m# need to rewrap column in Series to apply key function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7192\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Programs\\Python\\envs\\ML24\\Lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1907\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mother_axes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1908\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_level_reference\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1909\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_level_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1910\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1911\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1912\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1913\u001b[0m         \u001b[1;31m# Check for duplicates\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1914\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'precision'"
     ]
    }
   ],
   "source": [
    "for score in scorings:\n",
    "    scoring_filter = score\n",
    "    display(results[results.scoring==scoring_filter]\\\n",
    "                .sort_values(by=scoring_filter,ascending=False)\\\n",
    "                .drop('scoring',axis=1)\\\n",
    "                .style.format(precision=3)\\\n",
    "                    .set_caption('Results for scoring \"{}\"'.format(scoring_filter))\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Confusion matrices\n",
    "\n",
    "Use the `ConfusionMatrixDisplay` with the best model of each scoring to compare the predictions\n",
    "\n",
    "Repeat for every scoring:\n",
    "- filter the results for the current scoring\n",
    "- find the row with the best value of the scoring \n",
    "- display the confusion matrix with an appropriate title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for score in scorings:\n",
    "    scoring_filter = score\n",
    "    best_row = results.loc[results.scoring==scoring_filter,scoring_filter].idxmax(axis=0)\n",
    "    disp = ConfusionMatrixDisplay.from_estimator(X=X_test, y=y_test, estimator = clfs[best_row])\n",
    "    disp.ax_.set_title(\"Best Model for {}: {}\".format(score,results.at[best_row,'model']))"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "ML24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
